{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"figures/svtLogo.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Mathematical Optimization for Engineers</h1></center>\n",
    "<center><h2>Lab 3 - Gradient descent</h2></center>\n",
    "\n",
    "In this lab, we will implement: \n",
    "\n",
    "1. The steepest descent method with Armijo and Wolfe linesearch algorithms<br>\n",
    "<br>\n",
    "2. Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [autograd](https://github.com/HIPS/autograd) package by Prof. Ryan Adams' research group to calculate the gradient and Hessian of our objective function. <i>autograd</i> uses automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.22.4)\n",
      "Collecting autograd\n",
      "  Downloading autograd-1.5-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting future>=0.15.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=1d0dbb443fadb04d081e03bdf55c90169fc11d79d5fb7908efbb695259ba5f39\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/22/73/06/557dc4f4ef68179b9d763930d6eec26b88ed7c389b19588a1c\n",
      "Successfully built future\n",
      "Installing collected packages: future, autograd\n",
      "Successfully installed autograd-1.5 future-0.18.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# to calculate the inverse of the Hessian in Newton's method\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# to calculate the gradient and Hessian of the objective function\n",
    "from autograd import grad\n",
    "from autograd import hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x): \n",
    "    f = 0.5 * (x[0] ** 2 + 20 * x[1] ** 2)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return ((x[0]-1)**2 + 100*(x[1]-x[0]**2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x): \n",
    "    return - 1.0 / 6 * x[0]**6 + 1 / 4 *x[0]**4 + 2 * x[0]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_function(func, x): \n",
    "    return grad(func)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_function(func,x):\n",
    "    return hessian(func)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: (objective function, initial guess, step rule, c1, c2, initial alpha, optimality tolerance)\n",
    "def steepest_descent(function, x0, stepRule, c1, c2, alpha0, tol=0.01):     \n",
    "    \n",
    "    xCur = x0\n",
    "    fcur = function(xCur)\n",
    "    gCur = gradient_function(function,xCur)\n",
    "    \n",
    "    # norm of the gradient at initial point for optimality condition\n",
    "    nmg0 = np.linalg.norm(gradient_function(function, x0))\n",
    "    \n",
    "    # count number of steps taken by method\n",
    "    it = 0\n",
    "    \n",
    "    # accumulate number of iterations needed by linesearch algorithm in every step\n",
    "    ls_iter = 0\n",
    "    \n",
    "    # store iterates for plotting\n",
    "    xIter=[]\n",
    "    xIter.append(x0)\n",
    "    \n",
    "    while(np.linalg.norm(gCur)>tol*min(1,nmg0)): \n",
    "\n",
    "        it=it+1\n",
    "        \n",
    "        # calculate descent direction\n",
    "        direction = -gCur\n",
    "        \n",
    "        # calculate step-length\n",
    "        if (stepRule=='armijo'):\n",
    "            alpha, ls_ = armijo(function,xCur,direction, c1, alpha0)\n",
    "        elif (stepRule=='wolfe'):\n",
    "            alpha, ls_ = wolfe(function,xCur,direction, c1, c2, alpha0)\n",
    "        else:\n",
    "            alpha = 0.01\n",
    "        ls_iter = ls_iter + ls_\n",
    "        \n",
    "        # update current point\n",
    "        \n",
    "        xCur = xCur + alpha*direction\n",
    "        fCur = function(xCur)\n",
    "        gCur = gradient_function(function,xCur)\n",
    "        \n",
    "        xIter.append(xCur)\n",
    "\n",
    "    return xCur, fcur, it, xIter, ls_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: (objective function, initial guess, optimality tolerance)\n",
    "def newton_descent(function, x0, stepRule='no', tol=0.01): \n",
    "\n",
    "    xCur = x0\n",
    "    fcur = function(xCur)\n",
    "    gCur = gradient_function(function,xCur)\n",
    "    hCur = hessian_function(function,xCur)    \n",
    "    \n",
    "    # norm of the gradient at initial point for optimality condition\n",
    "    nmg0 = np.linalg.norm(gradient_function(function, x0))    \n",
    "    \n",
    "    # count number of steps taken by method    \n",
    "    it = 0\n",
    "    \n",
    "    # store iterates for plotting    \n",
    "    xIter=[]\n",
    "    xIter.append(x0)\n",
    "    \n",
    "    while(np.linalg.norm(gCur)>tol*min(1,nmg0)):\n",
    "        \n",
    "        it=it+1\n",
    "        \n",
    "        # calculate descent direction\n",
    "        direction = -1.0 * np.dot(inv(hCur),gCur)\n",
    "        \n",
    "        # calculate step-length or use natural length of 1\n",
    "        if (stepRule ==  'no'):\n",
    "            alpha = 1\n",
    "        else:\n",
    "            alpha = 0.01\n",
    "        \n",
    "        # update current point\n",
    "        xCur = xCur + alpha * direction\n",
    "        fCur = function(xCur)\n",
    "        gCur = gradient_function(function,xCur)\n",
    "        hCur = hessian_function(function,xCur)\n",
    "        \n",
    "        xIter.append(xCur)\n",
    "\n",
    "    return xCur, fcur, it, xIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo(function, xcur, searchdirection, c1, alpha0): \n",
    "    #f(x+alpha p) \\leq f(x) + c1 alpha grad(f).p\n",
    "    \n",
    "    alpha = alpha0\n",
    "    xnew = xcur + alpha * searchdirection\n",
    "    fcur = function(xcur)\n",
    "    fnew = function(xnew)\n",
    "    gradientCur = gradient_function(function,xcur)\n",
    "    numiter = 0\n",
    "    # check for Armijo condition \n",
    "    while (function(xnew) > function(xcur) + alpha*c1*np.dot(gradientCur,searchdirection)): \n",
    "        numiter += 1\n",
    "        alpha = alpha/2.0\n",
    "        xnew = xcur + alpha * searchdirection\n",
    "        fnew = function(xnew)\n",
    "\n",
    "    return alpha, numiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wolfe(function, xcur, searchdirection, c1, c2, alpha0): \n",
    "    #f(x+alpha p) \\leq f(x) + c1 alpha grad(f)Tp\n",
    "    #grad(f(x+alpha p))Tp \\geq c2 grad(f)Tp\n",
    "    \n",
    "    alpha = alpha0\n",
    "    xnew = xcur + alpha * searchdirection\n",
    "    fcur = function(xcur)\n",
    "    fnew = function(xnew)\n",
    "    gradientCur = gradient_function(function,xcur)\n",
    "    gradientNew = gradient_function(function,xnew)\n",
    "    numiter = 0\n",
    "    \n",
    "    lb = 0\n",
    "    ub = np.inf \n",
    "    \n",
    "    # check for Wolfe conditions\n",
    "    # adapted from https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf (pg. 8)\n",
    "    while True: \n",
    "        numiter += 1\n",
    "        if ((fnew) > (fcur + c1 * alpha * np.dot(gradientCur,searchdirection))):\n",
    "            ub = alpha\n",
    "            alpha = 0.5 * (lb + ub)\n",
    "        elif (np.dot(gradientNew,searchdirection) < c2 * np.dot(gradientCur,searchdirection)):\n",
    "            lb = alpha\n",
    "            if np.isinf(ub):\n",
    "                alpha = 2 * lb\n",
    "            else:\n",
    "                alpha = 0.5 * (lb + ub)\n",
    "        else:\n",
    "            break\n",
    "        xnew = xcur + alpha * searchdirection\n",
    "        fnew = function(xnew)\n",
    "        gradientNew = gradient_function(function,xnew)\n",
    "\n",
    "    return alpha, numiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving quadratic unconstrained problem\n",
    "###  Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method                                           xopt                 fopt              iter           ls_iter    \n",
      "Armijo with default tol = 0.01           [0.0069, -0.0001]          5.0500e-01           44              15             \n",
      "\n",
      "\n",
      "Wolfe with default tol = 0.01            [0.0069, -0.0002]          5.0500e-01           34              62             \n",
      "\n",
      "\n",
      "Armijo with tol = 0.0001                 [0.0001, -0.0000]          5.0500e-01           82              26             \n",
      "\n",
      "\n",
      "Wolfe with tol = 0.0001                  [0.0001, -0.0000]          5.0500e-01           64              114            \n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([0.9, 0.1])\n",
    "c1 = 0.5 \n",
    "c2 = 0.6 \n",
    "alpha0 = 0.125 \n",
    "tol = 0.0001\n",
    "\n",
    "print (\"{:<40} {:^20} {:^20} {:^15} {:^15}\".format('Method','xopt','fopt','iter','ls_iter'))\n",
    "\n",
    "# using steepest descent with Armijo condition for linesearch\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'armijo', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'wolfe', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'armijo', c1, c2, alpha0, tol)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'wolfe', c1, c2, alpha0, tol)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method                                           xopt                 fopt              iter      \n",
      "Newton with default tol = 0.01           [0.0000, 0.0000]          5.0500e-01           1              \n",
      "\n",
      "\n",
      "Newton with tol = 0.0001                 [0.0000, 0.0000]          5.0500e-01           1              \n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([0.9, 0.1])\n",
    "c1 = 0.5 \n",
    "c2 = 0.6 \n",
    "alpha0 = 0.125 \n",
    "tol = 0.0001\n",
    "\n",
    "print (\"{:<40} {:^20} {:^20} {:^15}\".format('Method','xopt','fopt','iter'))\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(objective_function, x0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it))\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(objective_function, x0, tol=0.0001)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Rosenbrock's function\n",
    "Try also different starting points and play around with the tuning factors.\n",
    "\n",
    "### Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method                                           xopt                 fopt              iter           ls_iter    \n",
      "Armijo with default tol = 0.01           [0.9892, 0.9784]          5.0420e+01           964             5156           \n",
      "\n",
      "\n",
      "Wolfe with default tol = 0.01            [0.9894, 0.9790]          5.0420e+01           960             6101           \n",
      "\n",
      "\n",
      "Armijo with tol = 0.0001                 [0.9999, 0.9998]          5.0420e+01           1434            7185           \n",
      "\n",
      "\n",
      "Wolfe with tol = 0.0001                  [0.9999, 0.9998]          5.0420e+01           1292            7896           \n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([0.9, 0.1])\n",
    "c1 = 0.5 \n",
    "c2 = 0.6 \n",
    "alpha0 = 0.125 \n",
    "tol = 0.0001\n",
    "\n",
    "print (\"{:<40} {:^20} {:^20} {:^15} {:^15}\".format('Method','xopt','fopt','iter','ls_iter'))\n",
    "\n",
    "# using steepest descent with Armijo condition for linesearch\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'armijo', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'wolfe', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'armijo', c1, c2, alpha0, tol)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'wolfe', c1, c2, alpha0, tol)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method                                           xopt                 fopt              iter      \n",
      "Newton with default tol = 0.01           [1.0000, 1.0000]          5.0420e+01           3              \n",
      "\n",
      "\n",
      "Newton with tol = 0.0001                 [1.0000, 1.0000]          5.0420e+01           3              \n"
     ]
    }
   ],
   "source": [
    "print (\"{:<40} {:^20} {:^20} {:^15}\".format('Method','xopt','fopt','iter'))\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(rosenbrock, x0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it))\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(rosenbrock, x0, tol=0.0001)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple polynomial function\n",
    "Here, a simple polynomial function is considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steepest descent with default tol = 0.01 [0.0018]          2.0833e+00           9              \n",
      "Newton with Armijo default tol = 0.01    [0.0025]          2.0833e+00           596            \n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([1.0])\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(polynomial, x0, 'armijo', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Steepest descent with default tol = 0.01',xopt[0],' ',fopt,it))\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(polynomial, x0, stepRule='armijo')\n",
    "print (\"{:<40} [{:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with Armijo default tol = 0.01',xopt[0],' ',fopt,it))\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(polynomial, x0)\n",
    "print (\"{:<40} [{:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],' ',fopt,it))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
