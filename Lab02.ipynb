{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"figures/svtLogo.png\"/>\n",
    "</div>\n",
    "<center><h1>Mathematical Optimization for Engineers</h1></center>\n",
    "<center><h2>Lab 2</h2></center>\n",
    "<center><h2>Basic math</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\mr}[1]{\\mathrm{#1}}\n",
    "\\newcommand{\\D}{\\displaystyle}\n",
    "\\newcommand{\\bm}[1]{\\text{\\mathbf $#1$}}\n",
    "\\newcommand{\\bx}{\\mathbf{ x}}\n",
    "\\newcommand{\\f}{\\mathbf{{f}}}\n",
    "\\newcommand{\\g}{\\mathbf{ g}}\n",
    "\\newcommand{\\h}{\\mathbf{ h}}\n",
    "\\newcommand{\\R}{\\mathbb R}\n",
    "\\newcommand{\\A}{\\mathbf{ A}}\n",
    "\\newcommand{\\br}{\\boldsymbol{r}}\n",
    "\\newcommand{\\bp}{\\boldsymbol{p}}\n",
    "\\newcommand{\\bnabla}{\\mathbf{\\nabla}}\n",
    "$$\n",
    "In this lab, we will learn about Jacobians, gradients and Hessian matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Notation</u>: Please note that throughout the course, we will denote matrices and vectors with boldface letters. Their components will be denoted by normal letters with subscripts. For example,\n",
    "$$\n",
    "\\bx = \\left(\\begin{array}{c}\n",
    "x_1 \\\\ \n",
    "\\vdots \\\\ \n",
    "x_n\n",
    "\\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian\n",
    "Let $\\ \\mathbf \\f:\\R^n \\rightarrow \\R^m,\\,\\bx \\mapsto \\f(\\bx)$ be a continuously differentiable function, where\n",
    "$$\n",
    "\\bx = \\left(\\begin{array}{c}\n",
    "x_1 \\\\ \n",
    "\\vdots \\\\ \n",
    "x_n\n",
    "\\end{array} \\right) , \\qquad \\f(\\bx)  = \\left(\\begin{array}{c}\n",
    "f_1(\\bx) \\\\ \n",
    "\\vdots \\\\ \n",
    "f_m(\\bx) \n",
    "\\end{array} \\right).\n",
    "$$\n",
    "The Jacobian $\\f'(\\bx)$ is defined by the matrix\n",
    "$$\n",
    "\\f'(\\bx) = \\left(  \\begin{array}{ccc}\n",
    "\\frac{\\partial f_1(\\bx)}{\\partial x_1} & \\cdots & \\frac{\\partial f_1(\\bx)}{\\partial x_n} \\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "\\frac{\\partial f_m(\\bx)}{\\partial x_1} & \\cdots & \\frac{\\partial f_m(\\bx)}{\\partial x_n}\n",
    "\\end{array}  \\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "Now, let $f:\\R^n \\rightarrow \\R,\\,\\bx \\mapsto f(\\bx)$ be a *scalar-valued* continuously differentiable function\n",
    "with vector-valued arguments.\n",
    "The gradient $\\bnabla f(\\bx)$ and Jacobian $f'(\\bx)$ are defined as the column vector and row vector, respectively,\n",
    "\n",
    "$$\n",
    "\\bnabla f(\\bx)  = \\left(\\begin{array}{c}\n",
    "\t\\frac{\\partial f(\\bx)}{\\partial x_1} \\\\ \n",
    "\t\\vdots \\\\ \n",
    "\t\\frac{\\partial f(\\bx)}{\\partial x_n}\n",
    "\\end{array} \\right), \\qquad  f'(\\bx)  = \\left(\\begin{array}{ccc}  \n",
    "\\frac{\\partial f(\\bx)}{\\partial x_1} & \n",
    "\\cdots & \n",
    "\\frac{\\partial f(\\bx)}{\\partial x_n}\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "Note that  $\\bnabla f(\\bx)^T=f'(\\bx)$.<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of the scalar product of two functions\n",
    "\n",
    "Let $\\g,\\h:\\R^n\\rightarrow\\R^n$ be two continuously differentiable functions. We want to compute the\n",
    "gradient of $f:\\R^n\\rightarrow \\R,\\;\\bx \\mapsto f(\\bx) = \\g(\\bx)^T\\h(\\bx)$, where $\\bx\\in\\R^n$.\n",
    "We have\n",
    "\n",
    "$$\n",
    "f(\\bx) = \\g(\\bx)^T\\h(\\bx) = \\sum_{i=1}^{n} g_i(\\bx)h_i(\\bx).\n",
    "$$\n",
    "\n",
    "The derivative with respect to $x_j$ ($1 \\le j \\le n$) is computed by the application of the product rule\n",
    "\n",
    "\\begin{equation}\\label{eq:1}\n",
    "\\frac{\\partial f(\\bx)}{\\partial x_j} = \\sum_{i=1}^{n} \\left(\\frac{\\partial g_i(\\bx)}{\\partial x_j}h_i(\\bx)  +g_i(\\bx)\\frac{\\partial h_i(\\bx)}{\\partial x_j}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "With the notations\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\g(\\bx)}{\\partial x_j}   = \\left(\\begin{array}{c}\n",
    "\\frac{\\partial g_1(\\bx)}{\\partial x_j} \\\\ \n",
    "\\vdots \\\\ \n",
    "\\frac{\\partial g_n(\\bx)}{\\partial x_j}\n",
    "\\end{array} \\right) \\quad \\text{and} \\quad \n",
    "\\frac{\\partial \\h(\\bx)}{\\partial x_j}   \n",
    "= \\left(\\begin{array}{c}\n",
    "\\frac{\\partial h_1(\\bx)}{\\partial x_j} \\\\ \n",
    "\\vdots \\\\ \n",
    "\\frac{\\partial h_n(\\bx)}{\\partial x_j}\n",
    "\\end{array} \\right), \\text{ respectively},\n",
    "$$\n",
    "\n",
    "we can rewrite the equation as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(\\bx)}{\\partial x_j} = \\frac{\\partial \\g(\\bx)}{\\partial x_j} ^T \\h(\\bx) + \n",
    "\\g(\\bx)^T \\frac{\\partial \\h(\\bx)}{\\partial x_j}  = \\h(\\bx)^T\\frac{\\partial \\g(\\bx)}{\\partial x_j}\n",
    "+\\g(\\bx)^T \\frac{\\partial \\h(\\bx)}{\\partial x_j}\n",
    "$$\n",
    "\n",
    "Finally,\n",
    "\n",
    "$$\n",
    "\\bnabla f(\\bx)^T = f'(\\bx) = \\h(\\bx)^T \\g'(\\bx) + \\g(\\bx)^T\\h'(\\bx).\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\bnabla f(\\bx) = \\bnabla g(\\bx) \\ \\h(\\bx) + \\bnabla h(\\bx) \\ \\g(\\bx).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of quadratic form\n",
    "If $\\A\\in\\R^{n\\times n}$ is a symmetric matrix, the function\n",
    "\n",
    "$$\n",
    "f:\\R^n\\rightarrow \\R,\\quad\\bx\\mapsto f(\\bx) = \\bx^T\\,\\A\\,\\bx\n",
    "$$ is called a quadratic form. <br>\n",
    "<br>\n",
    "With the definitions,\n",
    "\n",
    "$$\n",
    "\\g(\\bx) := \\bx \\ \\text{and } \\ \\h(\\bx):= \\A\\,\\bx,\n",
    "$$\n",
    "\n",
    "we have $f(\\bx) = \\g(\\bx)^T\\h(\\bx)$, i.e., exactly the situation as above.\n",
    "With $\\g'(\\bx) = \\mathbf{ I}$, where $\\mathbf{ I}$ denotes the unity matrix, and $\\h'(\\bx) = \\A$, it is more or less easy to see that the gradient of $f$ is given by\n",
    "\n",
    "$$\n",
    "\\bnabla f(\\bx) = 2\\,\\A\\,\\bx.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "Let the function $f:\\R^2 \\rightarrow \\R$ be defined as\n",
    "$ f(x,y) = \\frac{1}{2} (x^2 + \\alpha y^2), \\alpha \\in \\R $\n",
    "\n",
    "<u>Task 1</u>: Find all stationary points of the function $f$.\n",
    "\n",
    "\n",
    "<u>Task 2</u>: Calculate the Hessian of the function $f$ for arbitrary $x$ and $y$.\n",
    "\n",
    "\n",
    "<u>Task 3</u>: What are the eigenvalues of the Hessian of the function $f$ with respect to $x$ and $y$?\n",
    "\n",
    "\n",
    "<u>Task 4</u>: Characterize the stationary points for positive and negative $\\alpha$.\n",
    "\n",
    "\n",
    "<u>Task 5</u>: Characterize the convexity of the function for every $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: <br>\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial f}{\\partial x} &= x = 0 & \\Leftrightarrow && x = 0 \\\\\n",
    "\\frac{\\partial f}{\\partial y} &= \\alpha y = 0 & \\Leftrightarrow && y = 0\n",
    "\\end{align}$$\n",
    "\n",
    "Task 2: <br>\n",
    "$$\n",
    "H=\\begin{bmatrix}1 & 0\\\\0 & \\alpha\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Task 3: <br>\n",
    "$$\n",
    "\\lambda_1 = 1, \\lambda_2 = 1\n",
    "$$\n",
    "\n",
    "Task 4: <br>\n",
    "$$\n",
    "\\text{if } \\alpha > 0, \\text{ stationary point is a strict minimum} \\\\\n",
    "\\text{if } \\alpha < 0, \\text{ stationary point is a saddle point}\n",
    "$$\n",
    "\n",
    "Task 5: <br>\n",
    "$$\n",
    "\\text{if } \\alpha > 0, H \\text{ is positive definite, so } f \\text{ is strictly convex} \\\\\n",
    "\\text{if } \\alpha = 0, H \\text{ is positive semi-definite, so } f \\text{ is convex (not strictly)} \\\\\n",
    "\\text{if } \\alpha < 0, H \\text{ is positive indefinite, so } f \\text{ is neither convex, nor concave}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Rosenbrock function\n",
    "The Rosenbrock function is a famous test function used in optimization. <br>\n",
    "<br>\n",
    "It is defined by: \n",
    "$f:\\R^2 \\rightarrow \\R$, $ f(x,y) = (a-x)^2 + b(y-x^2)^2 $ with $a=1, b=100$\n",
    "\n",
    "<u>Task 1</u>: Find all stationary points of the function $f$.\n",
    "\n",
    "<u>Task 2</u>: Calculate the Hessian of the function $f$ for the stationary points found. (Hint: use Python)\n",
    "\n",
    "<u>Task 3</u>: What are the eigenvalues of the Hessian of the function $f$? (Hint: use Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: <br>\n",
    "$$\\begin{align}\n",
    "&& \\frac{\\partial f}{\\partial x} (1-x)^2 + 100 (y-x^2)^2 = 0\\\\\n",
    "& \\Leftrightarrow & 2(x-1) + 100 \\cdot 2 (y-x^2)(-2x) = 0\\\\\n",
    "& \\Leftrightarrow & 2(x-1) - 400x (y-x^2) = 0 \\\\\n",
    "\\\\\n",
    "&& \\frac{\\partial f}{\\partial y} (1-x)^2 + 100 (y-x^2)^2 = 0 \\\\\n",
    "& \\Leftrightarrow & 200 (y-x^2) = 0\n",
    "\\end{align}$$\n",
    "\n",
    "$$\n",
    "\\text{By substituting the second equation in the first, we get}\\\\\n",
    "2(x-1) = 0 \\Leftrightarrow x = 1 \\\\\n",
    "200 (y-x^2) = 0 \\Leftrightarrow 200 (y - 1) = 0 \\Leftrightarrow y = 1\n",
    "$$\n",
    "The stationary point is at (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.22.4)\n",
      "Collecting autograd\n",
      "  Downloading autograd-1.5-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting future>=0.15.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=273f14997878617760bbdefd67fd335107997a17d121ebbd91970b146fe43841\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/22/73/06/557dc4f4ef68179b9d763930d6eec26b88ed7c389b19588a1c\n",
      "Successfully built future\n",
      "Installing collected packages: future, autograd\n",
      "Successfully installed autograd-1.5 future-0.18.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return ((x[0]-1)**2 + 100*(x[1]-x[0]**2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 802. -400.]\n",
      " [-400.  200.]]\n"
     ]
    }
   ],
   "source": [
    "# compute hessian using autograd\n",
    "import autograd\n",
    "import numpy as np\n",
    "x0 = np.array([1.0, 1.0])\n",
    "hessian_rosenbrock = autograd.hessian(rosenbrock)(x0)\n",
    "print(hessian_rosenbrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00160064e+03, 3.99360767e-01])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the eigenvalues using numpy\n",
    "import numpy as np\n",
    "np.linalg.eig(hessian_rosenbrock)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
