{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"figures/svtLogo.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Mathematical Optimization for Engineers</h1></center>\n",
    "<center><h2>Lab 3 - Gradient descent</h2></center>\n",
    "\n",
    "In this lab, we will implement: \n",
    "\n",
    "1. The steepest descent method with Armijo and Wolfe linesearch algorithms<br>\n",
    "<br>\n",
    "2. Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [autograd](https://github.com/HIPS/autograd) package by Prof. Ryan Adams' research group to calculate the gradient and Hessian of our objective function. <i>autograd</i> uses automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# to calculate the inverse of the Hessian in Newton's method\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# to calculate the gradient and Hessian of the objective function\n",
    "from autograd import grad\n",
    "from autograd import hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x): \n",
    "    f = 0.5 * (x[0] ** 2 + 20 * x[1] ** 2)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return ((x[0]-1)**2 + 100*(x[1]-x[0]**2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_function(func, x): \n",
    "    return grad(func)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_function(func,x):\n",
    "    return hessian(func)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: (objective function, initial guess, step rule, c1, c2, initial alpha, optimality tolerance)\n",
    "def steepest_descent(function, x0, stepRule, c1, c2, alpha0, tol=0.01):     \n",
    "    \n",
    "    xCur = x0\n",
    "    fcur = function(xCur)\n",
    "    gCur = gradient_function(function,xCur)\n",
    "    \n",
    "    # norm of the gradient at initial point for optimality condition\n",
    "    nmg0 = np.linalg.norm(gradient_function(function, x0))\n",
    "    \n",
    "    # count number of steps taken by method\n",
    "    it = 0\n",
    "    \n",
    "    # accumulate number of iterations needed by linesearch algorithm in every step\n",
    "    ls_iter = 0\n",
    "    \n",
    "    # store iterates for plotting\n",
    "    xIter=[]\n",
    "    xIter.append(x0)\n",
    "    \n",
    "    while(np.linalg.norm(gCur)>tol*min(1,nmg0)): \n",
    "\n",
    "        it=it+1\n",
    "        \n",
    "        # calculate descent direction\n",
    "        direction = -1.0 * gradient_function(function,xCur)\n",
    "        \n",
    "        # calculate step-length\n",
    "        if (stepRule=='armijo'):\n",
    "            alpha, ls_ = armijo(function,xCur,direction, c1, alpha0)\n",
    "        elif (stepRule=='wolfe'):\n",
    "            alpha, ls_ = wolfe(function,xCur,direction, c1, c2, alpha0)\n",
    "        else:\n",
    "            alpha = 0.01\n",
    "        ls_iter = ls_iter + ls_\n",
    "        \n",
    "        # update current point\n",
    "        xCur = xCur + alpha * direction\n",
    "        gCur = gradient_function(function,xCur)\n",
    "        fcur = function(xCur)\n",
    "        xIter.append(xCur)\n",
    "\n",
    "    return xCur, fcur, it, xIter, ls_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: (objective function, initial guess, optimality tolerance)\n",
    "def newton_descent(function, x0, tol=0.01): \n",
    "\n",
    "    xCur = x0\n",
    "    fcur = function(xCur)\n",
    "    gCur = gradient_function(function,xCur)\n",
    "    hCur = hessian_function(function,xCur)    \n",
    "    \n",
    "    # norm of the gradient at initial point for optimality condition\n",
    "    nmg0 = np.linalg.norm(gradient_function(function, x0))    \n",
    "    \n",
    "    # count number of steps taken by method    \n",
    "    it = 0\n",
    "    \n",
    "    # store iterates for plotting    \n",
    "    xIter=[]\n",
    "    xIter.append(x0)\n",
    "    \n",
    "    while(np.linalg.norm(gCur)>tol*min(1,nmg0)):\n",
    "        \n",
    "        it=it+1\n",
    "        \n",
    "        # calculate descent direction\n",
    "        direction = -1.0 * np.dot(inv(hCur),gCur)\n",
    "        \n",
    "        # calculate step-length\n",
    "        alpha = 1.0\n",
    "        \n",
    "        # update current point\n",
    "        xCur = xCur + alpha * direction\n",
    "        fcur = function(xCur)        \n",
    "        gCur = gradient_function(function,xCur)\n",
    "        hCur = hessian_function(function,xCur)\n",
    "        xIter.append(xCur)\n",
    "\n",
    "    return xCur, fcur, it, xIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo(function, xcur, searchdirection, c1, alpha0): \n",
    "    #f(x+alpha p) \\leq f(x) + c1 alpha grad(f).p\n",
    "    \n",
    "    alpha = alpha0\n",
    "    xnew = xcur + alpha * searchdirection\n",
    "    fcur = function(xcur)\n",
    "    fnew = function(xnew)\n",
    "    gradientCur = gradient_function(function,xcur)\n",
    "    numiter = 0\n",
    "    # check for Armijo condition\n",
    "    while ((fnew) > (fcur + c1 * alpha * np.dot(gradientCur,searchdirection))): \n",
    "        numiter += 1\n",
    "        alpha = alpha/2.0\n",
    "        xnew = xcur + alpha * searchdirection\n",
    "        fnew = function(xnew)\n",
    "\n",
    "    return alpha, numiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wolfe(function, xcur, searchdirection, c1, c2, alpha0): \n",
    "    #f(x+alpha p) \\leq f(x) + c1 alpha grad(f)Tp\n",
    "    #grad(f(x+alpha p))Tp \\geq c2 grad(f)Tp\n",
    "    \n",
    "    alpha = alpha0\n",
    "    xnew = xcur + alpha * searchdirection\n",
    "    fcur = function(xcur)\n",
    "    fnew = function(xnew)\n",
    "    gradientCur = gradient_function(function,xcur)\n",
    "    gradientNew = gradient_function(function,xnew)\n",
    "    numiter = 0\n",
    "    \n",
    "    lb = 0\n",
    "    ub = np.inf \n",
    "    \n",
    "    # check for Wolfe conditions\n",
    "    # adapted from https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf (pg. 8)\n",
    "    while True: \n",
    "        numiter += 1\n",
    "        if ((fnew) > (fcur + c1 * alpha * np.dot(gradientCur,searchdirection))):\n",
    "            ub = alpha\n",
    "            alpha = 0.5 * (lb + ub)\n",
    "        elif (np.dot(gradientNew,searchdirection) < c2 * np.dot(gradientCur,searchdirection)):\n",
    "            lb = alpha\n",
    "            if np.isinf(ub):\n",
    "                alpha = 2 * lb\n",
    "            else:\n",
    "                alpha = 0.5 * (lb + ub)\n",
    "        else:\n",
    "            break\n",
    "        xnew = xcur + alpha * searchdirection\n",
    "        fnew = function(xnew)\n",
    "        gradientNew = gradient_function(function,xnew)\n",
    "\n",
    "    return alpha, numiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving quadratic unconstrained problem\n",
    "###  Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method                                           xopt                 fopt              iter           ls_iter    \n",
      "Armijo with default tol = 0.01           [0.0069, -0.0001]          2.3644e-05           44              15             \n",
      "\n",
      "\n",
      "Wolfe with default tol = 0.01            [0.0069, -0.0002]          2.3811e-05           34              62             \n",
      "\n",
      "\n",
      "Armijo with tol = 0.0001                 [0.0001, -0.0000]          4.2227e-09           82              26             \n",
      "\n",
      "\n",
      "Wolfe with tol = 0.0001                  [0.0001, -0.0000]          4.5868e-09           64              114            \n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([0.9, 0.1])\n",
    "c1 = 0.5 \n",
    "c2 = 0.6 \n",
    "alpha0 = 0.125 \n",
    "tol = 0.0001\n",
    "\n",
    "print (\"{:<40} {:^20} {:^20} {:^15} {:^15}\".format('Method','xopt','fopt','iter','ls_iter'))\n",
    "\n",
    "# using steepest descent with Armijo condition for linesearch\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'armijo', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'wolfe', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'armijo', c1, c2, alpha0, tol)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'wolfe', c1, c2, alpha0, tol)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method                                           xopt                 fopt              iter      \n",
      "Newton with default tol = 0.01           [0.0000, 0.0000]          0.0000e+00           1              \n",
      "\n",
      "\n",
      "Newton with tol = 0.0001                 [0.0000, 0.0000]          0.0000e+00           1              \n"
     ]
    }
   ],
   "source": [
    "print (\"{:<40} {:^20} {:^20} {:^15}\".format('Method','xopt','fopt','iter'))\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(objective_function, x0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it))\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(objective_function, x0, tol=0.0001)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Rosenbrock's function\n",
    "### Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method                                           xopt                 fopt              iter           ls_iter    \n",
      "Armijo with default tol = 0.01           [0.9892, 0.9784]          1.1708e-04           964             5156           \n",
      "\n",
      "\n",
      "Wolfe with default tol = 0.01            [0.9894, 0.9790]          1.1162e-04           960             6101           \n",
      "\n",
      "\n",
      "Armijo with tol = 0.0001                 [0.9999, 0.9998]          1.1294e-08           1434            7185           \n",
      "\n",
      "\n",
      "Wolfe with tol = 0.0001                  [0.9999, 0.9998]          1.1923e-08           1292            7896           \n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([0.9, 0.1])\n",
    "c1 = 0.5 \n",
    "c2 = 0.6 \n",
    "alpha0 = 0.125 \n",
    "tol = 0.0001\n",
    "\n",
    "print (\"{:<40} {:^20} {:^20} {:^15} {:^15}\".format('Method','xopt','fopt','iter','ls_iter'))\n",
    "\n",
    "# using steepest descent with Armijo condition for linesearch\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'armijo', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'wolfe', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'armijo', c1, c2, alpha0, tol)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'wolfe', c1, c2, alpha0, tol)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method                                           xopt                 fopt              iter      \n",
      "Newton with default tol = 0.01           [1.0000, 1.0000]          4.1516e-11           3              \n",
      "\n",
      "\n",
      "Newton with tol = 0.0001                 [1.0000, 1.0000]          4.1516e-11           3              \n"
     ]
    }
   ],
   "source": [
    "print (\"{:<40} {:^20} {:^20} {:^15}\".format('Method','xopt','fopt','iter'))\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(rosenbrock, x0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it))\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(rosenbrock, x0, tol=0.0001)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
